{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to run pyspark from jupyter notebook you should run next command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"YourAppName\") \\\n",
    "#     .config(\"spark.driver.extraClassPath\", \"/path/to/postgresql-42.6.0.jar\") \\\n",
    "#     .config(\"spark.executor.extraClassPath\", \"/path/to/postgresql-42.6.0.jar\") \\\n",
    "#     .config(\"spark.jars\", \"/path/to/postgresql-42.6.0.jar\") \\\n",
    "#     .config(\"spark.driver.extraJavaOptions\", \"-Duser.timezone=UTC\") \\\n",
    "#     .config(\"spark.executor.extraJavaOptions\", \"-Duser.timezone=UTC\") \\\n",
    "#     .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, from_unixtime, date_format, when, round, lit, lag, unix_timestamp, concat, monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e174c3-348a-45ee-b43c-047b9e6f38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_file_path = 'events.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9cdfa09-914c-4250-8567-8fc2e059067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_schema = StructType([\n",
    "    StructField(\"event_id\", IntegerType(), True),\n",
    "    StructField(\"event_timestamp\", IntegerType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3890a6-d13d-43f0-b1c3-cf3b87359afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df = spark.read.schema(check_schema).json(events_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking are all event ids unique in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c30e01-763a-4da4-b5b9-b3df76514586",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_event_ids_count = check_df.select(\"event_id\").distinct().count()\n",
    "if unique_event_ids_count == check_df.count():\n",
    "    print(\"All event_id values are unique.\")\n",
    "else:\n",
    "    print(\"Some event_id values are not unique.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking are all event types registration, transaction, login, logout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d438c3fa-af7d-4c9a-9ccb-f7f5a7a8a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_event_types = {\"registration\", \"transaction\", \"login\", \"logout\"}\n",
    "distinct_event_types = set(check_df.select(\"event_type\").distinct().rdd.flatMap(lambda x: x).collect())\n",
    "if distinct_event_types.issubset(expected_event_types):\n",
    "    print(\"All event_type values are within the expected set.\")\n",
    "else:\n",
    "    print(\"Some event_type values are not within the expected set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c4f7af-110e-4ec5-8cf5-1113ed0d0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal is to create 3 tables, one for registration which will contain information about users, second is transaction which will contain informations about transactions and third is session, which will contain information about login and logout events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f83c91-fb50-4377-a1db-443a937425db",
   "metadata": {},
   "outputs": [],
   "source": [
    "registration_schema = StructType([\n",
    "    StructField(\"event_id\", IntegerType(), True),\n",
    "    StructField(\"event_timestamp\", IntegerType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"event_data\", StructType([\n",
    "        StructField(\"country\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"user_id\", StringType(), True),\n",
    "        StructField(\"device_os\", StringType(), True),\n",
    "        StructField(\"marketing_campaign\", StringType(), True),\n",
    "    ]), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef92b92c-12eb-49ec-be81-65fbf4795ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_schema = StructType([\n",
    "    StructField(\"event_id\", IntegerType(), True),\n",
    "    StructField(\"event_timestamp\", IntegerType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"event_data\", StructType([\n",
    "        StructField(\"user_id\", StringType(), True),\n",
    "        StructField(\"transaction_currency\", StringType(), True),\n",
    "        StructField(\"transaction_amount\", FloatType(), True),\n",
    "    ]), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25435ebd-4743-46e8-ba8a-9ad3f2214c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_schema = StructType([\n",
    "    StructField(\"event_id\", IntegerType(), True),\n",
    "    StructField(\"event_timestamp\", IntegerType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"event_data\", StructType([\n",
    "        StructField(\"user_id\", StringType(), True),\n",
    "    ]), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each table schema is created so when is loaded from original dataset it only contains informations relevant for that table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537da257-a330-4dab-a78c-96b82a02b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "registration_df = spark.read.schema(registration_schema).json(events_file_path)\n",
    "registration_df = registration_df.filter(registration_df.event_type == \"registration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb08ef-fd44-4832-9129-42ae8b736e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df = spark.read.schema(transaction_schema).json(events_file_path)\n",
    "transaction_df = transaction_df.filter(transaction_df.event_type == \"transaction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cca38d-82f3-4471-be32-5980a264ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_df = spark.read.schema(session_schema).json(events_file_path)\n",
    "session_df = session_df.filter(col(\"event_type\").isin(\"login\", \"logout\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registration table contains columns: user_id which is also primary key for this table, date of registration, time of registration, name of user, country of user, device_os and marketing_campaign which are not relevant for this challenge but are left in table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96abdf2e-f4b4-48b9-bc15-24d1159682d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+----------+-------------------+--------------------------+-------+---------+------------------------+\n",
      "|user_id                             |date      |time               |name                      |country|device_os|marketing_campaign      |\n",
      "+------------------------------------+----------+-------------------+--------------------------+-------+---------+------------------------+\n",
      "|63faa66c-683b-11ee-aca7-8699b86be788|2010-05-18|2023-11-18 00:12:07|Lope Cañellas             |ES     |Android  |influencer_marketing    |\n",
      "|846202ce-683b-11ee-aca7-8699b86be788|2010-05-22|2023-11-18 13:27:51|Dr. Ulrich Hauffer        |DE     |iOS      |NULL                    |\n",
      "|6422113e-683b-11ee-aca7-8699b86be788|2010-05-18|2023-11-18 23:49:44|Mr. Daniel Taylor         |US     |Android  |NULL                    |\n",
      "|6fc81560-683b-11ee-aca7-8699b86be788|2010-05-20|2023-11-18 07:50:38|Horst Meyer               |DE     |iOS      |social_media_advertising|\n",
      "|65494f0a-683b-11ee-aca7-8699b86be788|2010-05-19|2023-11-18 15:11:31|Jorge Poza-Soto           |ES     |Android  |social_media_advertising|\n",
      "|6f2bdc22-683b-11ee-aca7-8699b86be788|2010-05-20|2023-11-18 18:13:27|Johanne Schmidt           |DE     |Web      |email_marketing         |\n",
      "|81bcbc9e-683b-11ee-aca7-8699b86be788|2010-05-22|2023-11-18 16:22:52|Ing. Erich Ehlert         |DE     |Web      |social_media_advertising|\n",
      "|822798f2-683b-11ee-aca7-8699b86be788|2010-05-22|2023-11-18 01:04:29|Riccardo Bernardi         |IT     |Web      |NULL                    |\n",
      "|870bc2c6-683b-11ee-aca7-8699b86be788|2010-05-22|2023-11-18 15:21:38|Ulises Dalmau             |ES     |iOS      |social_media_advertising|\n",
      "|6f0367ec-683b-11ee-aca7-8699b86be788|2010-05-20|2023-11-18 13:57:34|Marcos Aragón Guardiola   |ES     |Web      |NULL                    |\n",
      "|76a429c8-683b-11ee-aca7-8699b86be788|2010-05-21|2023-11-18 22:17:02|Ortrun Hande              |DE     |Web      |social_media_advertising|\n",
      "|5f09226e-683b-11ee-aca7-8699b86be788|2010-05-15|2023-11-18 22:00:39|Dipl.-Ing. Mirella Ziegert|DE     |Android  |social_media_advertising|\n",
      "|74cd0fd4-683b-11ee-aca7-8699b86be788|2010-05-21|2023-11-18 14:33:40|Prof. Kazim Noack         |DE     |iOS      |social_media_advertising|\n",
      "|80fb8fe2-683b-11ee-aca7-8699b86be788|2010-05-22|2023-11-18 17:48:29|Barbara Mullen            |US     |Android  |social_media_advertising|\n",
      "|7c5658f0-683b-11ee-aca7-8699b86be788|2010-05-22|2023-11-18 23:29:33|Dawn Taylor               |US     |Android  |influencer_marketing    |\n",
      "|77d580da-683b-11ee-aca7-8699b86be788|2010-05-21|2023-11-18 01:17:11|Amy Richard               |US     |Android  |influencer_marketing    |\n",
      "|6864c958-683b-11ee-aca7-8699b86be788|2010-05-19|2023-11-18 06:27:48|Berenice Naccari          |IT     |Android  |influencer_marketing    |\n",
      "|69663d00-683b-11ee-aca7-8699b86be788|2010-05-20|2023-11-18 16:09:31|Natali Binner             |DE     |iOS      |NULL                    |\n",
      "|6a7f0e74-683b-11ee-aca7-8699b86be788|2010-05-20|2023-11-18 00:15:17|Lidia Dörr MBA.           |DE     |Android  |NULL                    |\n",
      "|6292c868-683b-11ee-aca7-8699b86be788|2010-05-18|2023-11-18 18:38:35|Gabriella Ossola          |IT     |Web      |NULL                    |\n",
      "+------------------------------------+----------+-------------------+--------------------------+-------+---------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 16:28:44 INFO FileSourceStrategy: Pushed Filters: IsNotNull(event_type),EqualTo(event_type,registration)\n",
      "23/11/18 16:28:44 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(event_type#43),(event_type#43 = registration)\n",
      "23/11/18 16:28:44 INFO CodeGenerator: Code generated in 15.371391 ms\n",
      "23/11/18 16:28:44 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 355.5 KiB, free 364.3 MiB)\n",
      "23/11/18 16:28:44 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 364.3 MiB)\n",
      "23/11/18 16:28:44 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.26.138.14:44159 (size: 35.1 KiB, free: 366.1 MiB)\n",
      "23/11/18 16:28:44 INFO SparkContext: Created broadcast 20 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/11/18 16:28:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/11/18 16:28:44 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/11/18 16:28:44 INFO DAGScheduler: Got job 12 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/11/18 16:28:44 INFO DAGScheduler: Final stage: ResultStage 17 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/11/18 16:28:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/11/18 16:28:44 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/18 16:28:44 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[52] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/11/18 16:28:44 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 23.0 KiB, free 364.3 MiB)\n",
      "23/11/18 16:28:44 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 364.3 MiB)\n",
      "23/11/18 16:28:44 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.26.138.14:44159 (size: 9.8 KiB, free: 366.1 MiB)\n",
      "23/11/18 16:28:44 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1580\n",
      "23/11/18 16:28:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[52] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/11/18 16:28:44 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0\n",
      "23/11/18 16:28:44 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (172.26.138.14, executor driver, partition 0, PROCESS_LOCAL, 8718 bytes) \n",
      "23/11/18 16:28:44 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)\n",
      "23/11/18 16:28:44 INFO CodeGenerator: Code generated in 16.327055 ms\n",
      "23/11/18 16:28:44 INFO FileScanRDD: Reading File path: file:///home/markov/nordeusProject/events.jsonl, range: 0-4194304, partition values: [empty row]\n",
      "23/11/18 16:28:44 INFO CodeGenerator: Code generated in 5.700142 ms\n",
      "23/11/18 16:28:44 INFO CodeGenerator: Code generated in 4.812987 ms\n",
      "23/11/18 16:28:44 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 3106 bytes result sent to driver\n",
      "23/11/18 16:28:44 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 57 ms on 172.26.138.14 (executor driver) (1/1)\n",
      "23/11/18 16:28:44 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "23/11/18 16:28:44 INFO DAGScheduler: ResultStage 17 (showString at NativeMethodAccessorImpl.java:0) finished in 0.062 s\n",
      "23/11/18 16:28:44 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/11/18 16:28:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished\n",
      "23/11/18 16:28:44 INFO DAGScheduler: Job 12 finished: showString at NativeMethodAccessorImpl.java:0, took 0.065762 s\n",
      "23/11/18 16:28:44 INFO CodeGenerator: Code generated in 5.945633 ms\n"
     ]
    }
   ],
   "source": [
    "new_registration_df = (\n",
    "    registration_df\n",
    "    .withColumn(\"date\", date_format(from_unixtime(col(\"event_timestamp\")), \"yyyy-MM-dd\"))\n",
    "    .withColumn(\"time\", date_format(from_unixtime(col(\"event_timestamp\")), \"HH:mm:ss\"))\n",
    "    .select(\n",
    "        col(\"event_data.user_id\").alias(\"user_id\"),\n",
    "        col(\"date\"),\n",
    "        col(\"time\"),\n",
    "        col(\"event_data.name\").alias(\"name\"),\n",
    "        col(\"event_data.country\").alias(\"country\"),\n",
    "        col(\"event_data.device_os\").alias(\"device_os\"),\n",
    "        col(\"event_data.marketing_campaign\").alias(\"marketing_campaign\")\n",
    "    )\n",
    ")\n",
    "\n",
    "new_registration_df = new_registration_df.withColumn(\n",
    "    \"marketing_campaign\",\n",
    "    when(col(\"marketing_campaign\") == '', None).otherwise(col(\"marketing_campaign\"))\n",
    ")\n",
    "new_registration_df = new_registration_df.withColumn(\"date\", col(\"date\").cast(DateType()))\n",
    "new_registration_df = new_registration_df.withColumn(\"time\", col(\"time\").cast(TimestampType()))\n",
    "new_registration_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transactions table contains columns: user_id which is foreign_key connected with table Registration, transaction_currency which is USD for every row, transaction_amount (other currency that were present in original dataset is EUR but everyting is converted to USD), data of transaction, time of transaction and transaction_id which is added so that it can be primary key for this table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ee31b7e-c158-4ff5-bc48-847cec26cd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+--------------------+------------------+----------+-------------------+--------------+\n",
      "|user_id                             |transaction_currency|transaction_amount|date      |time               |transaction_id|\n",
      "+------------------------------------+--------------------+------------------+----------+-------------------+--------------+\n",
      "|81cebb24-683b-11ee-aca7-8699b86be788|USD                 |1.99              |2010-05-22|2023-11-18 23:55:19|0             |\n",
      "|734ed624-683b-11ee-aca7-8699b86be788|USD                 |0.99              |2010-05-22|2023-11-18 23:25:10|1             |\n",
      "|6017708e-683b-11ee-aca7-8699b86be788|USD                 |2.99              |2010-05-22|2023-11-18 23:23:46|2             |\n",
      "|74b67418-683b-11ee-aca7-8699b86be788|USD                 |0.99              |2010-05-21|2023-11-18 23:58:40|3             |\n",
      "|603a31fa-683b-11ee-aca7-8699b86be788|USD                 |1.29              |2010-05-17|2023-11-18 07:10:10|4             |\n",
      "|833f9b40-683b-11ee-aca7-8699b86be788|USD                 |3.89              |2010-05-22|2023-11-18 23:24:57|5             |\n",
      "|615750e0-683b-11ee-aca7-8699b86be788|USD                 |1.29              |2010-05-18|2023-11-18 20:36:42|6             |\n",
      "|6c896912-683b-11ee-aca7-8699b86be788|USD                 |3.89              |2010-05-20|2023-11-18 21:05:24|7             |\n",
      "|5ced4c8a-683b-11ee-aca7-8699b86be788|USD                 |1.29              |2010-05-17|2023-11-18 22:37:08|8             |\n",
      "|79bfd30a-683b-11ee-aca7-8699b86be788|USD                 |3.89              |2010-05-22|2023-11-18 23:35:08|9             |\n",
      "|8967c844-683b-11ee-aca7-8699b86be788|USD                 |0.99              |2010-05-22|2023-11-18 23:52:34|10            |\n",
      "|5d57dbc2-683b-11ee-aca7-8699b86be788|USD                 |2.99              |2010-05-19|2023-11-18 23:24:32|11            |\n",
      "|755d533c-683b-11ee-aca7-8699b86be788|USD                 |1.29              |2010-05-22|2023-11-18 23:58:42|12            |\n",
      "|88f2c210-683b-11ee-aca7-8699b86be788|USD                 |2.99              |2010-05-22|2023-11-18 23:46:31|13            |\n",
      "|6832bf76-683b-11ee-aca7-8699b86be788|USD                 |3.89              |2010-05-21|2023-11-18 18:15:14|14            |\n",
      "|5d5c7704-683b-11ee-aca7-8699b86be788|USD                 |1.29              |2010-05-21|2023-11-18 23:42:53|15            |\n",
      "|79bfd30a-683b-11ee-aca7-8699b86be788|USD                 |3.89              |2010-05-22|2023-11-18 23:55:32|16            |\n",
      "|6190ecce-683b-11ee-aca7-8699b86be788|USD                 |3.89              |2010-05-19|2023-11-18 23:52:51|17            |\n",
      "|7da4322c-683b-11ee-aca7-8699b86be788|USD                 |2.59              |2010-05-22|2023-11-18 23:52:16|18            |\n",
      "|62caba3e-683b-11ee-aca7-8699b86be788|USD                 |0.99              |2010-05-22|2023-11-18 23:50:13|19            |\n",
      "+------------------------------------+--------------------+------------------+----------+-------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 16:28:44 INFO FileSourceStrategy: Pushed Filters: IsNotNull(event_type),EqualTo(event_type,transaction)\n",
      "23/11/18 16:28:44 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(event_type#52),(event_type#52 = transaction)\n",
      "23/11/18 16:28:44 INFO CodeGenerator: Code generated in 19.721691 ms\n",
      "23/11/18 16:28:44 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 355.5 KiB, free 363.9 MiB)\n",
      "23/11/18 16:28:44 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 363.9 MiB)\n",
      "23/11/18 16:28:44 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.26.138.14:44159 (size: 35.1 KiB, free: 366.1 MiB)\n",
      "23/11/18 16:28:44 INFO SparkContext: Created broadcast 22 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/11/18 16:28:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/11/18 16:28:44 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/11/18 16:28:44 INFO DAGScheduler: Got job 13 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/11/18 16:28:44 INFO DAGScheduler: Final stage: ResultStage 18 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/11/18 16:28:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/11/18 16:28:44 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/18 16:28:44 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[56] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/11/18 16:28:44 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 23.5 KiB, free 363.9 MiB)\n",
      "23/11/18 16:28:44 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 10.1 KiB, free 363.8 MiB)\n",
      "23/11/18 16:28:44 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.26.138.14:44159 (size: 10.1 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:28:44 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1580\n",
      "23/11/18 16:28:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[56] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/11/18 16:28:44 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
      "23/11/18 16:28:44 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (172.26.138.14, executor driver, partition 0, PROCESS_LOCAL, 8718 bytes) \n",
      "23/11/18 16:28:44 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)\n",
      "23/11/18 16:28:44 INFO CodeGenerator: Code generated in 21.160352 ms\n",
      "23/11/18 16:28:44 INFO FileScanRDD: Reading File path: file:///home/markov/nordeusProject/events.jsonl, range: 0-4194304, partition values: [empty row]\n",
      "23/11/18 16:28:44 INFO CodeGenerator: Code generated in 7.678376 ms\n",
      "23/11/18 16:28:44 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 2414 bytes result sent to driver\n",
      "23/11/18 16:28:44 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 57 ms on 172.26.138.14 (executor driver) (1/1)\n",
      "23/11/18 16:28:44 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "23/11/18 16:28:44 INFO DAGScheduler: ResultStage 18 (showString at NativeMethodAccessorImpl.java:0) finished in 0.064 s\n",
      "23/11/18 16:28:44 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/11/18 16:28:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished\n",
      "23/11/18 16:28:44 INFO DAGScheduler: Job 13 finished: showString at NativeMethodAccessorImpl.java:0, took 0.067183 s\n",
      "23/11/18 16:28:44 INFO CodeGenerator: Code generated in 6.307775 ms\n"
     ]
    }
   ],
   "source": [
    "conversion_rate = 1.3\n",
    "new_transaction_df = (\n",
    "    transaction_df\n",
    "    .select(\n",
    "        col(\"event_data.user_id\").alias(\"user_id\"),\n",
    "        lit(\"USD\").alias(\"transaction_currency\"),\n",
    "        round(\n",
    "            when(col(\"event_data.transaction_currency\") == \"EUR\",\n",
    "                 col(\"event_data.transaction_amount\") * conversion_rate)\n",
    "            .otherwise(col(\"event_data.transaction_amount\")), 2\n",
    "        ).alias(\"transaction_amount\"),\n",
    "        date_format(from_unixtime(col(\"event_timestamp\")), \"yyyy-MM-dd\").alias(\"date\"),\n",
    "        date_format(from_unixtime(col(\"event_timestamp\")), \"HH:mm:ss\").alias(\"time\")\n",
    "    )\n",
    ")\n",
    "# Show the new DataFrame\n",
    "new_transaction_df = new_transaction_df.withColumn(\"transaction_id\", monotonically_increasing_id())\n",
    "new_transaction_df = new_transaction_df.withColumn(\"date\", col(\"date\").cast(DateType()))\n",
    "new_transaction_df = new_transaction_df.withColumn(\"time\", col(\"time\").cast(TimestampType()))\n",
    "new_transaction_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table Session is created in a way so that each row represents one session by having login date and time, and logout date and time, and duration in seconds which represents number of seconds between login and logout. In original dataset it was noticed that for same user there are multiple login or logout events in a row, which doesn't make sense, so only first event of login or logout for that array of multiple same events are kept. Also column named session_id is added so that it can be primary key for this table, and user_id is foreign key connected with table Registration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transaction_df.count()\n",
    "from pyspark.sql.functions import min, max\n",
    "print(new_transaction_df.select(max('transaction_amount')).collect())\n",
    "new_transaction_df.select(min('transaction_amount')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a477b94-6ae3-4279-b159-f0751b9a4914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 16:28:44 INFO FileSourceStrategy: Pushed Filters: In(event_type, [login,logout])\n",
      "23/11/18 16:28:44 INFO FileSourceStrategy: Post-Scan Filters: event_type#61 IN (login,logout)\n",
      "23/11/18 16:28:45 INFO CodeGenerator: Code generated in 5.481222 ms\n",
      "23/11/18 16:28:45 INFO CodeGenerator: Code generated in 8.895111 ms\n",
      "23/11/18 16:28:45 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 355.5 KiB, free 363.5 MiB)\n",
      "23/11/18 16:28:45 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 363.5 MiB)\n",
      "23/11/18 16:28:45 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.26.138.14:44159 (size: 35.1 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:28:45 INFO SparkContext: Created broadcast 24 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/11/18 16:28:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/11/18 16:28:45 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Got job 14 (showString at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Final stage: ResultStage 19 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[61] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/11/18 16:28:45 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 22.0 KiB, free 363.4 MiB)\n",
      "23/11/18 16:28:45 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 9.9 KiB, free 363.4 MiB)\n",
      "23/11/18 16:28:45 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.26.138.14:44159 (size: 9.9 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:28:45 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1580\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 19 (MapPartitionsRDD[61] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "23/11/18 16:28:45 INFO TaskSchedulerImpl: Adding task set 19.0 with 2 tasks resource profile 0\n",
      "23/11/18 16:28:45 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (172.26.138.14, executor driver, partition 0, PROCESS_LOCAL, 8718 bytes) \n",
      "23/11/18 16:28:45 INFO TaskSetManager: Starting task 1.0 in stage 19.0 (TID 20) (172.26.138.14, executor driver, partition 1, PROCESS_LOCAL, 8718 bytes) \n",
      "23/11/18 16:28:45 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)\n",
      "23/11/18 16:28:45 INFO Executor: Running task 1.0 in stage 19.0 (TID 20)\n",
      "23/11/18 16:28:45 INFO CodeGenerator: Code generated in 5.136464 ms\n",
      "23/11/18 16:28:45 INFO CodeGenerator: Code generated in 10.45968 ms\n",
      "23/11/18 16:28:45 INFO FileScanRDD: Reading File path: file:///home/markov/nordeusProject/events.jsonl, range: 4194304-7668804, partition values: [empty row]\n",
      "23/11/18 16:28:45 INFO FileScanRDD: Reading File path: file:///home/markov/nordeusProject/events.jsonl, range: 0-4194304, partition values: [empty row]\n",
      "23/11/18 16:28:45 INFO CodeGenerator: Code generated in 5.162142 ms\n",
      "23/11/18 16:28:45 INFO CodeGenerator: Code generated in 4.730823 ms\n",
      "23/11/18 16:28:45 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.26.138.14:44159 in memory (size: 35.1 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:28:45 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.26.138.14:44159 in memory (size: 10.1 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:28:45 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.26.138.14:44159 in memory (size: 35.1 KiB, free: 366.1 MiB)\n",
      "23/11/18 16:28:45 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.26.138.14:44159 in memory (size: 9.8 KiB, free: 366.1 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+----------+----------+--------+\n",
      "|user_id                             |event_type|date      |time    |\n",
      "+------------------------------------+----------+----------+--------+\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|login     |2010-05-08|14:40:34|\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|logout    |2010-05-08|14:41:53|\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|login     |2010-05-08|16:50:15|\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|logout    |2010-05-08|16:50:45|\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|login     |2010-05-08|18:11:35|\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|logout    |2010-05-08|18:15:03|\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|login     |2010-05-08|19:06:41|\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|logout    |2010-05-08|19:10:53|\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|login     |2010-05-09|19:02:26|\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|logout    |2010-05-09|19:09:43|\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|login     |2010-05-09|19:58:59|\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|logout    |2010-05-09|20:07:40|\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|login     |2010-05-10|21:12:40|\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|logout    |2010-05-10|21:20:39|\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|login     |2010-05-11|22:49:32|\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|logout    |2010-05-11|22:50:04|\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|login     |2010-05-12|23:46:21|\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|logout    |2010-05-12|23:47:12|\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|login     |2010-05-12|23:56:56|\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|logout    |2010-05-13|00:01:49|\n",
      "+------------------------------------+----------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 16:28:45 INFO Executor: Finished task 1.0 in stage 19.0 (TID 20). 4320 bytes result sent to driver\n",
      "23/11/18 16:28:45 INFO TaskSetManager: Finished task 1.0 in stage 19.0 (TID 20) in 260 ms on 172.26.138.14 (executor driver) (1/2)\n",
      "23/11/18 16:28:45 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 4320 bytes result sent to driver\n",
      "23/11/18 16:28:45 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 284 ms on 172.26.138.14 (executor driver) (2/2)\n",
      "23/11/18 16:28:45 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "23/11/18 16:28:45 INFO DAGScheduler: ResultStage 19 (showString at NativeMethodAccessorImpl.java:0) finished in 0.289 s\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/11/18 16:28:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Job 14 finished: showString at NativeMethodAccessorImpl.java:0, took 0.293362 s\n",
      "23/11/18 16:28:45 INFO CodeGenerator: Code generated in 5.413165 ms\n"
     ]
    }
   ],
   "source": [
    "new_session_df = (\n",
    "    session_df\n",
    "    .select(\n",
    "        col(\"event_data.user_id\").alias(\"user_id\"),\n",
    "        col(\"event_type\"),\n",
    "        date_format(from_unixtime(col(\"event_timestamp\")), \"yyyy-MM-dd\").alias(\"date\"),\n",
    "        date_format(from_unixtime(col(\"event_timestamp\")), \"HH:mm:ss\").alias(\"time\")\n",
    "    )\n",
    "    .orderBy(\"user_id\", \"date\", \"time\")\n",
    ")\n",
    "\n",
    "# Show the new DataFrame\n",
    "new_session_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "308bd7bb-89c8-4e76-a3da-55221a7edeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 16:28:45 INFO FileSourceStrategy: Pushed Filters: In(event_type, [login,logout])\n",
      "23/11/18 16:28:45 INFO FileSourceStrategy: Post-Scan Filters: event_type#61 IN (login,logout)\n",
      "23/11/18 16:28:45 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 355.5 KiB, free 363.9 MiB)\n",
      "23/11/18 16:28:45 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 363.9 MiB)\n",
      "23/11/18 16:28:45 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.26.138.14:44159 (size: 35.1 KiB, free: 366.1 MiB)\n",
      "23/11/18 16:28:45 INFO SparkContext: Created broadcast 26 from collect at /tmp/ipykernel_533/1496398248.py:1\n",
      "23/11/18 16:28:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/11/18 16:28:45 INFO CodeGenerator: Code generated in 4.256873 ms\n",
      "23/11/18 16:28:45 INFO SparkContext: Starting job: collect at /tmp/ipykernel_533/1496398248.py:1\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Got job 15 (collect at /tmp/ipykernel_533/1496398248.py:1) with 2 output partitions\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Final stage: ResultStage 20 (collect at /tmp/ipykernel_533/1496398248.py:1)\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[67] at collect at /tmp/ipykernel_533/1496398248.py:1), which has no missing parents\n",
      "23/11/18 16:28:45 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 22.1 KiB, free 363.9 MiB)\n",
      "23/11/18 16:28:45 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 10.1 KiB, free 363.9 MiB)\n",
      "23/11/18 16:28:45 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.26.138.14:44159 (size: 10.1 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:28:45 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1580\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 20 (MapPartitionsRDD[67] at collect at /tmp/ipykernel_533/1496398248.py:1) (first 15 tasks are for partitions Vector(0, 1))\n",
      "23/11/18 16:28:45 INFO TaskSchedulerImpl: Adding task set 20.0 with 2 tasks resource profile 0\n",
      "23/11/18 16:28:45 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 21) (172.26.138.14, executor driver, partition 0, PROCESS_LOCAL, 8718 bytes) \n",
      "23/11/18 16:28:45 INFO TaskSetManager: Starting task 1.0 in stage 20.0 (TID 22) (172.26.138.14, executor driver, partition 1, PROCESS_LOCAL, 8718 bytes) \n",
      "23/11/18 16:28:45 INFO Executor: Running task 0.0 in stage 20.0 (TID 21)\n",
      "23/11/18 16:28:45 INFO Executor: Running task 1.0 in stage 20.0 (TID 22)\n",
      "23/11/18 16:28:45 INFO CodeGenerator: Code generated in 3.991024 ms\n",
      "23/11/18 16:28:45 INFO FileScanRDD: Reading File path: file:///home/markov/nordeusProject/events.jsonl, range: 0-4194304, partition values: [empty row]\n",
      "23/11/18 16:28:45 INFO FileScanRDD: Reading File path: file:///home/markov/nordeusProject/events.jsonl, range: 4194304-7668804, partition values: [empty row]\n",
      "23/11/18 16:28:45 INFO MemoryStore: Block taskresult_22 stored as bytes in memory (estimated size 1991.6 KiB, free 361.9 MiB)\n",
      "23/11/18 16:28:45 INFO BlockManagerInfo: Added taskresult_22 in memory on 172.26.138.14:44159 (size: 1991.6 KiB, free: 364.1 MiB)\n",
      "23/11/18 16:28:45 INFO Executor: Finished task 1.0 in stage 20.0 (TID 22). 2039446 bytes result sent via BlockManager)\n",
      "23/11/18 16:28:45 INFO MemoryStore: Block taskresult_21 stored as bytes in memory (estimated size 2.4 MiB, free 359.6 MiB)\n",
      "23/11/18 16:28:45 INFO BlockManagerInfo: Added taskresult_21 in memory on 172.26.138.14:44159 (size: 2.4 MiB, free: 361.7 MiB)\n",
      "23/11/18 16:28:45 INFO Executor: Finished task 0.0 in stage 20.0 (TID 21). 2466179 bytes result sent via BlockManager)\n",
      "23/11/18 16:28:45 INFO TransportClientFactory: Successfully created connection to /172.26.138.14:44159 after 3 ms (0 ms spent in bootstraps)\n",
      "23/11/18 16:28:45 INFO TaskSetManager: Finished task 1.0 in stage 20.0 (TID 22) in 286 ms on 172.26.138.14 (executor driver) (1/2)\n",
      "23/11/18 16:28:45 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 21) in 286 ms on 172.26.138.14 (executor driver) (2/2)\n",
      "23/11/18 16:28:45 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "23/11/18 16:28:45 INFO DAGScheduler: ResultStage 20 (collect at /tmp/ipykernel_533/1496398248.py:1) finished in 0.298 s\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/11/18 16:28:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Job 15 finished: collect at /tmp/ipykernel_533/1496398248.py:1, took 0.305130 s\n",
      "23/11/18 16:28:45 INFO BlockManagerInfo: Removed taskresult_22 on 172.26.138.14:44159 in memory (size: 1991.6 KiB, free: 363.7 MiB)\n",
      "23/11/18 16:28:45 INFO BlockManagerInfo: Removed taskresult_21 on 172.26.138.14:44159 in memory (size: 2.4 MiB, free: 366.0 MiB)\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Registering RDD 68 (collect at /tmp/ipykernel_533/1496398248.py:1) as input to shuffle 4\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Got map stage job 16 (collect at /tmp/ipykernel_533/1496398248.py:1) with 2 output partitions\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Final stage: ShuffleMapStage 21 (collect at /tmp/ipykernel_533/1496398248.py:1)\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[68] at collect at /tmp/ipykernel_533/1496398248.py:1), which has no missing parents\n",
      "23/11/18 16:28:45 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 46.2 KiB, free 363.8 MiB)\n",
      "23/11/18 16:28:45 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 15.4 KiB, free 363.8 MiB)\n",
      "23/11/18 16:28:45 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.26.138.14:44159 (size: 15.4 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:28:45 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1580\n",
      "23/11/18 16:28:45 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[68] at collect at /tmp/ipykernel_533/1496398248.py:1) (first 15 tasks are for partitions Vector(0, 1))\n",
      "23/11/18 16:28:45 INFO TaskSchedulerImpl: Adding task set 21.0 with 2 tasks resource profile 0\n",
      "23/11/18 16:28:45 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 23) (172.26.138.14, executor driver, partition 0, PROCESS_LOCAL, 8707 bytes) \n",
      "23/11/18 16:28:45 INFO TaskSetManager: Starting task 1.0 in stage 21.0 (TID 24) (172.26.138.14, executor driver, partition 1, PROCESS_LOCAL, 8707 bytes) \n",
      "23/11/18 16:28:45 INFO Executor: Running task 0.0 in stage 21.0 (TID 23)\n",
      "23/11/18 16:28:45 INFO Executor: Running task 1.0 in stage 21.0 (TID 24)\n",
      "23/11/18 16:28:45 INFO CodeGenerator: Code generated in 6.67114 ms\n",
      "23/11/18 16:28:45 INFO FileScanRDD: Reading File path: file:///home/markov/nordeusProject/events.jsonl, range: 4194304-7668804, partition values: [empty row]\n",
      "23/11/18 16:28:45 INFO FileScanRDD: Reading File path: file:///home/markov/nordeusProject/events.jsonl, range: 0-4194304, partition values: [empty row]\n",
      "23/11/18 16:28:46 INFO Executor: Finished task 1.0 in stage 21.0 (TID 24). 2089 bytes result sent to driver\n",
      "23/11/18 16:28:46 INFO TaskSetManager: Finished task 1.0 in stage 21.0 (TID 24) in 236 ms on 172.26.138.14 (executor driver) (1/2)\n",
      "23/11/18 16:28:46 INFO Executor: Finished task 0.0 in stage 21.0 (TID 23). 2089 bytes result sent to driver\n",
      "23/11/18 16:28:46 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 23) in 250 ms on 172.26.138.14 (executor driver) (2/2)\n",
      "23/11/18 16:28:46 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "23/11/18 16:28:46 INFO DAGScheduler: ShuffleMapStage 21 (collect at /tmp/ipykernel_533/1496398248.py:1) finished in 0.263 s\n",
      "23/11/18 16:28:46 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/11/18 16:28:46 INFO DAGScheduler: running: Set()\n",
      "23/11/18 16:28:46 INFO DAGScheduler: waiting: Set()\n",
      "23/11/18 16:28:46 INFO DAGScheduler: failed: Set()\n",
      "23/11/18 16:28:46 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "23/11/18 16:28:46 INFO CodeGenerator: Code generated in 8.615402 ms\n",
      "23/11/18 16:28:46 INFO SparkContext: Starting job: collect at /tmp/ipykernel_533/1496398248.py:1\n",
      "23/11/18 16:28:46 INFO DAGScheduler: Got job 17 (collect at /tmp/ipykernel_533/1496398248.py:1) with 1 output partitions\n",
      "23/11/18 16:28:46 INFO DAGScheduler: Final stage: ResultStage 23 (collect at /tmp/ipykernel_533/1496398248.py:1)\n",
      "23/11/18 16:28:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)\n",
      "23/11/18 16:28:46 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/18 16:28:46 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[71] at collect at /tmp/ipykernel_533/1496398248.py:1), which has no missing parents\n",
      "23/11/18 16:28:46 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 36.7 KiB, free 363.8 MiB)\n",
      "23/11/18 16:28:46 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 363.7 MiB)\n",
      "23/11/18 16:28:46 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.26.138.14:44159 (size: 17.6 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:28:46 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1580\n",
      "23/11/18 16:28:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[71] at collect at /tmp/ipykernel_533/1496398248.py:1) (first 15 tasks are for partitions Vector(0))\n",
      "23/11/18 16:28:46 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0\n",
      "23/11/18 16:28:46 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 25) (172.26.138.14, executor driver, partition 0, NODE_LOCAL, 8108 bytes) \n",
      "23/11/18 16:28:46 INFO Executor: Running task 0.0 in stage 23.0 (TID 25)\n",
      "23/11/18 16:28:46 INFO ShuffleBlockFetcherIterator: Getting 2 (815.4 KiB) non-empty blocks including 2 (815.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/11/18 16:28:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "23/11/18 16:28:46 INFO CodeGenerator: Code generated in 6.434116 ms\n",
      "23/11/18 16:28:46 INFO CodeGenerator: Code generated in 5.478864 ms\n",
      "23/11/18 16:28:46 INFO Executor: Finished task 0.0 in stage 23.0 (TID 25). 609735 bytes result sent to driver\n",
      "23/11/18 16:28:46 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 25) in 118 ms on 172.26.138.14 (executor driver) (1/1)\n",
      "23/11/18 16:28:46 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "23/11/18 16:28:46 INFO DAGScheduler: ResultStage 23 (collect at /tmp/ipykernel_533/1496398248.py:1) finished in 0.126 s\n",
      "23/11/18 16:28:46 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/11/18 16:28:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished\n",
      "23/11/18 16:28:46 INFO DAGScheduler: Job 17 finished: collect at /tmp/ipykernel_533/1496398248.py:1, took 0.133556 s\n"
     ]
    }
   ],
   "source": [
    "session_collect = new_session_df.collect()\n",
    "result_rows = []\n",
    "old_user_id = \"5c55c572-683b-11ee-aca7-8699b86be788\"\n",
    "old_event_type = \"\"\n",
    "old_date = None\n",
    "old_time = None\n",
    "counter = 0\n",
    "for row in session_collect:\n",
    "    if row.user_id != old_user_id:\n",
    "        result_rows.append((old_user_id, old_date, old_time, None, None))\n",
    "    elif row.user_id == old_user_id and row.event_type == \"logout\" and old_event_type == \"login\":\n",
    "        result_rows.append((row.user_id, old_date, old_time, row.date, row.time))\n",
    "    old_user_id = row.user_id\n",
    "    old_event_type = row.event_type\n",
    "    old_date = row.date\n",
    "    old_time = row.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b2f083d-8037-42dd-b7e9-2411e49c2fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20458"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fc68857-a2b6-4c71-b990-2858ea6b68de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+-----------+-----------+\n",
      "|             user_id|login_date|login_time|logout_date|logout_time|\n",
      "+--------------------+----------+----------+-----------+-----------+\n",
      "|5c55c572-683b-11e...|2010-05-08|  14:40:34| 2010-05-08|   14:41:53|\n",
      "|5c55c572-683b-11e...|2010-05-08|  16:50:15| 2010-05-08|   16:50:45|\n",
      "|5c55c572-683b-11e...|2010-05-08|  18:11:35| 2010-05-08|   18:15:03|\n",
      "|5c55c572-683b-11e...|2010-05-08|  19:06:41| 2010-05-08|   19:10:53|\n",
      "|5c55c572-683b-11e...|2010-05-09|  19:02:26| 2010-05-09|   19:09:43|\n",
      "|5c55c572-683b-11e...|2010-05-09|  19:58:59| 2010-05-09|   20:07:40|\n",
      "|5c55c572-683b-11e...|2010-05-10|  21:12:40| 2010-05-10|   21:20:39|\n",
      "|5c55c572-683b-11e...|2010-05-11|  22:49:32| 2010-05-11|   22:50:04|\n",
      "|5c55c572-683b-11e...|2010-05-12|  23:46:21| 2010-05-12|   23:47:12|\n",
      "|5c55c572-683b-11e...|2010-05-12|  23:56:56| 2010-05-13|   00:01:49|\n",
      "|5c55c572-683b-11e...|2010-05-15|  23:52:24| 2010-05-15|   23:53:20|\n",
      "|5c55c572-683b-11e...|2010-05-15|  23:58:44| 2010-05-16|   00:04:27|\n",
      "|5c55c572-683b-11e...|2010-05-17|  23:59:55| 2010-05-18|   00:01:49|\n",
      "|5c55c572-683b-11e...|2010-05-18|  13:39:47| 2010-05-18|   13:45:46|\n",
      "|5c55c572-683b-11e...|2010-05-19|  23:54:30| 2010-05-20|   00:03:23|\n",
      "|5c55c572-683b-11e...|2010-05-22|  23:57:58| 2010-05-22|   23:58:33|\n",
      "|5c55c572-683b-11e...|2010-05-22|  23:59:47| 2010-05-23|   00:03:07|\n",
      "|5c55c572-683b-11e...|2010-05-23|  00:09:07|       NULL|       NULL|\n",
      "|5c585d32-683b-11e...|2010-05-10|  19:55:25| 2010-05-10|   19:57:32|\n",
      "|5c585d32-683b-11e...|2010-05-10|  20:18:08| 2010-05-10|   20:21:13|\n",
      "+--------------------+----------+----------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 16:28:47 INFO CodeGenerator: Code generated in 5.723432 ms\n",
      "23/11/18 16:28:47 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/11/18 16:28:47 INFO DAGScheduler: Got job 18 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/11/18 16:28:47 INFO DAGScheduler: Final stage: ResultStage 24 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/11/18 16:28:47 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/11/18 16:28:47 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/18 16:28:47 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[78] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/11/18 16:28:47 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 13.7 KiB, free 363.7 MiB)\n",
      "23/11/18 16:28:47 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 363.7 MiB)\n",
      "23/11/18 16:28:47 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.26.138.14:44159 (size: 6.8 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:28:47 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1580\n",
      "23/11/18 16:28:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[78] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/11/18 16:28:47 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0\n",
      "23/11/18 16:28:47 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 26) (172.26.138.14, executor driver, partition 0, PROCESS_LOCAL, 95166 bytes) \n",
      "23/11/18 16:28:47 INFO Executor: Running task 0.0 in stage 24.0 (TID 26)\n",
      "23/11/18 16:28:47 INFO CodeGenerator: Code generated in 5.177297 ms\n",
      "23/11/18 16:28:47 INFO Executor: Finished task 0.0 in stage 24.0 (TID 26). 2397 bytes result sent to driver\n",
      "23/11/18 16:28:47 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 26) in 104 ms on 172.26.138.14 (executor driver) (1/1)\n",
      "23/11/18 16:28:47 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "23/11/18 16:28:47 INFO DAGScheduler: ResultStage 24 (showString at NativeMethodAccessorImpl.java:0) finished in 0.115 s\n",
      "23/11/18 16:28:47 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/11/18 16:28:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished\n",
      "23/11/18 16:28:47 INFO DAGScheduler: Job 18 finished: showString at NativeMethodAccessorImpl.java:0, took 0.120564 s\n",
      "23/11/18 16:28:47 INFO CodeGenerator: Code generated in 6.101421 ms\n"
     ]
    }
   ],
   "source": [
    "new_session_df = spark.createDataFrame(result_rows, [\"user_id\", \"login_date\", \"login_time\", \"logout_date\", \"logout_time\"])\n",
    "\n",
    "# Show the new DataFrame\n",
    "new_session_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dab3dc9-abdc-4f28-8adb-49c272d3dc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 16:28:47 INFO CodeGenerator: Code generated in 15.77225 ms\n",
      "23/11/18 16:28:47 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/11/18 16:28:47 INFO DAGScheduler: Got job 19 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/11/18 16:28:47 INFO DAGScheduler: Final stage: ResultStage 25 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/11/18 16:28:47 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/11/18 16:28:47 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/18 16:28:47 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[80] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/11/18 16:28:47 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 20.1 KiB, free 363.7 MiB)\n",
      "23/11/18 16:28:47 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 363.7 MiB)\n",
      "23/11/18 16:28:47 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.26.138.14:44159 (size: 9.0 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:28:47 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1580\n",
      "23/11/18 16:28:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[80] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/11/18 16:28:47 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0\n",
      "23/11/18 16:28:47 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 27) (172.26.138.14, executor driver, partition 0, PROCESS_LOCAL, 95166 bytes) \n",
      "23/11/18 16:28:47 INFO Executor: Running task 0.0 in stage 25.0 (TID 27)\n",
      "23/11/18 16:28:47 INFO CodeGenerator: Code generated in 15.952368 ms\n",
      "23/11/18 16:28:48 INFO Executor: Finished task 0.0 in stage 25.0 (TID 27). 2804 bytes result sent to driver\n",
      "23/11/18 16:28:48 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 27) in 107 ms on 172.26.138.14 (executor driver) (1/1)\n",
      "23/11/18 16:28:48 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "23/11/18 16:28:48 INFO DAGScheduler: ResultStage 25 (showString at NativeMethodAccessorImpl.java:0) finished in 0.114 s\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/11/18 16:28:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Job 19 finished: showString at NativeMethodAccessorImpl.java:0, took 0.119077 s\n",
      "23/11/18 16:28:48 INFO CodeGenerator: Code generated in 4.744534 ms\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Registering RDD 82 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 5\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Got map stage job 20 (count at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Final stage: ShuffleMapStage 26 (count at NativeMethodAccessorImpl.java:0)\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[82] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/11/18 16:28:48 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 16.0 KiB, free 363.7 MiB)\n",
      "23/11/18 16:28:48 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 363.7 MiB)\n",
      "23/11/18 16:28:48 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.26.138.14:44159 (size: 8.4 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:28:48 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1580\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[82] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "23/11/18 16:28:48 INFO TaskSchedulerImpl: Adding task set 26.0 with 12 tasks resource profile 0\n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 28) (172.26.138.14, executor driver, partition 0, PROCESS_LOCAL, 95155 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 1.0 in stage 26.0 (TID 29) (172.26.138.14, executor driver, partition 1, PROCESS_LOCAL, 177651 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 2.0 in stage 26.0 (TID 30) (172.26.138.14, executor driver, partition 2, PROCESS_LOCAL, 172723 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 3.0 in stage 26.0 (TID 31) (172.26.138.14, executor driver, partition 3, PROCESS_LOCAL, 89182 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 4.0 in stage 26.0 (TID 32) (172.26.138.14, executor driver, partition 4, PROCESS_LOCAL, 166592 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 5.0 in stage 26.0 (TID 33) (172.26.138.14, executor driver, partition 5, PROCESS_LOCAL, 163793 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 6.0 in stage 26.0 (TID 34) (172.26.138.14, executor driver, partition 6, PROCESS_LOCAL, 83419 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 7.0 in stage 26.0 (TID 35) (172.26.138.14, executor driver, partition 7, PROCESS_LOCAL, 155929 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 8.0 in stage 26.0 (TID 36) (172.26.138.14, executor driver, partition 8, PROCESS_LOCAL, 149867 bytes) \n",
      "23/11/18 16:28:48 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 172.26.138.14:44159 in memory (size: 9.0 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 9.0 in stage 26.0 (TID 37) (172.26.138.14, executor driver, partition 9, PROCESS_LOCAL, 79379 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 10.0 in stage 26.0 (TID 38) (172.26.138.14, executor driver, partition 10, PROCESS_LOCAL, 143137 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 11.0 in stage 26.0 (TID 39) (172.26.138.14, executor driver, partition 11, PROCESS_LOCAL, 137805 bytes) \n",
      "23/11/18 16:28:48 INFO Executor: Running task 0.0 in stage 26.0 (TID 28)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 1.0 in stage 26.0 (TID 29)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 2.0 in stage 26.0 (TID 30)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 3.0 in stage 26.0 (TID 31)\n",
      "23/11/18 16:28:48 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.26.138.14:44159 in memory (size: 17.6 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 4.0 in stage 26.0 (TID 32)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 5.0 in stage 26.0 (TID 33)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 6.0 in stage 26.0 (TID 34)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 7.0 in stage 26.0 (TID 35)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 8.0 in stage 26.0 (TID 36)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 9.0 in stage 26.0 (TID 37)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 11.0 in stage 26.0 (TID 39)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 10.0 in stage 26.0 (TID 38)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+----------+-------------------+-----------+-------------------+----------------+----------+\n",
      "|user_id                             |login_date|login_time         |logout_date|logout_time        |duration_seconds|session_id|\n",
      "+------------------------------------+----------+-------------------+-----------+-------------------+----------------+----------+\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|2010-05-08|2023-11-18 14:40:34|2010-05-08 |2023-11-18 14:41:53|79              |0         |\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|2010-05-08|2023-11-18 16:50:15|2010-05-08 |2023-11-18 16:50:45|30              |1         |\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|2010-05-08|2023-11-18 18:11:35|2010-05-08 |2023-11-18 18:15:03|208             |2         |\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|2010-05-08|2023-11-18 19:06:41|2010-05-08 |2023-11-18 19:10:53|252             |3         |\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|2010-05-09|2023-11-18 19:02:26|2010-05-09 |2023-11-18 19:09:43|437             |4         |\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|2010-05-09|2023-11-18 19:58:59|2010-05-09 |2023-11-18 20:07:40|521             |5         |\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|2010-05-10|2023-11-18 21:12:40|2010-05-10 |2023-11-18 21:20:39|479             |6         |\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|2010-05-11|2023-11-18 22:49:32|2010-05-11 |2023-11-18 22:50:04|32              |7         |\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|2010-05-12|2023-11-18 23:46:21|2010-05-12 |2023-11-18 23:47:12|51              |8         |\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|2010-05-12|2023-11-18 23:56:56|2010-05-13 |2023-11-18 00:01:49|293             |9         |\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|2010-05-15|2023-11-18 23:52:24|2010-05-15 |2023-11-18 23:53:20|56              |10        |\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|2010-05-15|2023-11-18 23:58:44|2010-05-16 |2023-11-18 00:04:27|343             |11        |\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|2010-05-17|2023-11-18 23:59:55|2010-05-18 |2023-11-18 00:01:49|114             |12        |\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|2010-05-18|2023-11-18 13:39:47|2010-05-18 |2023-11-18 13:45:46|359             |13        |\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|2010-05-19|2023-11-18 23:54:30|2010-05-20 |2023-11-18 00:03:23|533             |14        |\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|2010-05-22|2023-11-18 23:57:58|2010-05-22 |2023-11-18 23:58:33|35              |15        |\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|2010-05-22|2023-11-18 23:59:47|2010-05-23 |2023-11-18 00:03:07|200             |16        |\n",
      "|5c55c572-683b-11ee-aca7-8699b86be788|2010-05-23|2023-11-18 00:09:07|NULL       |NULL               |NULL            |17        |\n",
      "|5c585d32-683b-11ee-aca7-8699b86be788|2010-05-10|2023-11-18 19:55:25|2010-05-10 |2023-11-18 19:57:32|127             |18        |\n",
      "|5c585d32-683b-11ee-aca7-8699b86be788|2010-05-10|2023-11-18 20:18:08|2010-05-10 |2023-11-18 20:21:13|185             |19        |\n",
      "+------------------------------------+----------+-------------------+-----------+-------------------+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 16:28:48 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.26.138.14:44159 in memory (size: 6.8 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:28:48 INFO CodeGenerator: Code generated in 16.049492 ms\n",
      "23/11/18 16:28:48 INFO PythonRunner: Times: total = 183, boot = 4, init = 153, finish = 26\n",
      "23/11/18 16:28:48 INFO Executor: Finished task 0.0 in stage 26.0 (TID 28). 2277 bytes result sent to driver\n",
      "23/11/18 16:28:48 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 28) in 218 ms on 172.26.138.14 (executor driver) (1/12)\n",
      "23/11/18 16:28:48 INFO PythonRunner: Times: total = 214, boot = 4, init = 205, finish = 5\n",
      "23/11/18 16:28:48 INFO Executor: Finished task 6.0 in stage 26.0 (TID 34). 2277 bytes result sent to driver\n",
      "23/11/18 16:28:48 INFO TaskSetManager: Finished task 6.0 in stage 26.0 (TID 34) in 263 ms on 172.26.138.14 (executor driver) (2/12)\n",
      "23/11/18 16:28:48 INFO PythonRunner: Times: total = 193, boot = 4, init = 169, finish = 20\n",
      "23/11/18 16:28:48 INFO PythonRunner: Times: total = 197, boot = 17, init = 171, finish = 9\n",
      "23/11/18 16:28:48 INFO Executor: Finished task 2.0 in stage 26.0 (TID 30). 2277 bytes result sent to driver\n",
      "23/11/18 16:28:48 INFO TaskSetManager: Finished task 2.0 in stage 26.0 (TID 30) in 280 ms on 172.26.138.14 (executor driver) (3/12)\n",
      "23/11/18 16:28:48 INFO PythonRunner: Times: total = 221, boot = 13, init = 203, finish = 5\n",
      "23/11/18 16:28:48 INFO PythonRunner: Times: total = 240, boot = 9, init = 228, finish = 3\n",
      "23/11/18 16:28:48 INFO Executor: Finished task 4.0 in stage 26.0 (TID 32). 2277 bytes result sent to driver\n",
      "23/11/18 16:28:48 INFO PythonRunner: Times: total = 241, boot = 36, init = 196, finish = 9\n",
      "23/11/18 16:28:48 INFO PythonRunner: Times: total = 233, boot = 10, init = 215, finish = 8\n",
      "23/11/18 16:28:48 INFO Executor: Finished task 3.0 in stage 26.0 (TID 31). 2277 bytes result sent to driver\n",
      "23/11/18 16:28:48 INFO TaskSetManager: Finished task 4.0 in stage 26.0 (TID 32) in 291 ms on 172.26.138.14 (executor driver) (4/12)\n",
      "23/11/18 16:28:48 INFO Executor: Finished task 10.0 in stage 26.0 (TID 38). 2277 bytes result sent to driver\n",
      "23/11/18 16:28:48 INFO Executor: Finished task 11.0 in stage 26.0 (TID 39). 2277 bytes result sent to driver\n",
      "23/11/18 16:28:48 INFO Executor: Finished task 5.0 in stage 26.0 (TID 33). 2277 bytes result sent to driver\n",
      "23/11/18 16:28:48 INFO PythonRunner: Times: total = 218, boot = 12, init = 187, finish = 19\n",
      "23/11/18 16:28:48 INFO TaskSetManager: Finished task 10.0 in stage 26.0 (TID 38) in 281 ms on 172.26.138.14 (executor driver) (5/12)\n",
      "23/11/18 16:28:48 INFO TaskSetManager: Finished task 3.0 in stage 26.0 (TID 31) in 300 ms on 172.26.138.14 (executor driver) (6/12)\n",
      "23/11/18 16:28:48 INFO TaskSetManager: Finished task 11.0 in stage 26.0 (TID 39) in 289 ms on 172.26.138.14 (executor driver) (7/12)\n",
      "23/11/18 16:28:48 INFO TaskSetManager: Finished task 5.0 in stage 26.0 (TID 33) in 302 ms on 172.26.138.14 (executor driver) (8/12)\n",
      "23/11/18 16:28:48 INFO Executor: Finished task 1.0 in stage 26.0 (TID 29). 2277 bytes result sent to driver\n",
      "23/11/18 16:28:48 INFO TaskSetManager: Finished task 1.0 in stage 26.0 (TID 29) in 308 ms on 172.26.138.14 (executor driver) (9/12)\n",
      "23/11/18 16:28:48 INFO PythonRunner: Times: total = 273, boot = 45, init = 225, finish = 3\n",
      "23/11/18 16:28:48 INFO Executor: Finished task 7.0 in stage 26.0 (TID 35). 2277 bytes result sent to driver\n",
      "23/11/18 16:28:48 INFO TaskSetManager: Finished task 7.0 in stage 26.0 (TID 35) in 304 ms on 172.26.138.14 (executor driver) (10/12)\n",
      "23/11/18 16:28:48 INFO PythonRunner: Times: total = 242, boot = 29, init = 195, finish = 18\n",
      "23/11/18 16:28:48 INFO Executor: Finished task 8.0 in stage 26.0 (TID 36). 2277 bytes result sent to driver\n",
      "23/11/18 16:28:48 INFO TaskSetManager: Finished task 8.0 in stage 26.0 (TID 36) in 317 ms on 172.26.138.14 (executor driver) (11/12)\n",
      "23/11/18 16:28:48 INFO PythonRunner: Times: total = 289, boot = 35, init = 253, finish = 1\n",
      "23/11/18 16:28:48 INFO Executor: Finished task 9.0 in stage 26.0 (TID 37). 2277 bytes result sent to driver\n",
      "23/11/18 16:28:48 INFO TaskSetManager: Finished task 9.0 in stage 26.0 (TID 37) in 334 ms on 172.26.138.14 (executor driver) (12/12)\n",
      "23/11/18 16:28:48 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
      "23/11/18 16:28:48 INFO DAGScheduler: ShuffleMapStage 26 (count at NativeMethodAccessorImpl.java:0) finished in 0.353 s\n",
      "23/11/18 16:28:48 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/11/18 16:28:48 INFO DAGScheduler: running: Set()\n",
      "23/11/18 16:28:48 INFO DAGScheduler: waiting: Set()\n",
      "23/11/18 16:28:48 INFO DAGScheduler: failed: Set()\n",
      "23/11/18 16:28:48 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Got job 21 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Final stage: ResultStage 28 (count at NativeMethodAccessorImpl.java:0)\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[85] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/11/18 16:28:48 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 12.5 KiB, free 363.8 MiB)\n",
      "23/11/18 16:28:48 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 363.7 MiB)\n",
      "23/11/18 16:28:48 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.26.138.14:44159 (size: 5.9 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:28:48 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1580\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[85] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/11/18 16:28:48 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0\n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 40) (172.26.138.14, executor driver, partition 0, NODE_LOCAL, 8108 bytes) \n",
      "23/11/18 16:28:48 INFO Executor: Running task 0.0 in stage 28.0 (TID 40)\n",
      "23/11/18 16:28:48 INFO ShuffleBlockFetcherIterator: Getting 12 (720.0 B) non-empty blocks including 12 (720.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/11/18 16:28:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "23/11/18 16:28:48 INFO Executor: Finished task 0.0 in stage 28.0 (TID 40). 3995 bytes result sent to driver\n",
      "23/11/18 16:28:48 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 40) in 7 ms on 172.26.138.14 (executor driver) (1/1)\n",
      "23/11/18 16:28:48 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "23/11/18 16:28:48 INFO DAGScheduler: ResultStage 28 (count at NativeMethodAccessorImpl.java:0) finished in 0.013 s\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/11/18 16:28:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Job 21 finished: count at NativeMethodAccessorImpl.java:0, took 0.015348 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20458"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_session_df = new_session_df.withColumn(\n",
    "    \"duration_seconds\",\n",
    "    (unix_timestamp(concat(col(\"logout_date\"), lit(\" \"), col(\"logout_time\")), \"yyyy-MM-dd HH:mm:ss\") -\n",
    "     unix_timestamp(concat(col(\"login_date\"), lit(\" \"), col(\"login_time\")), \"yyyy-MM-dd HH:mm:ss\"))\n",
    ")\n",
    "new_session_df = new_session_df.withColumn(\"session_id\", monotonically_increasing_id())\n",
    "new_session_df = new_session_df.withColumn(\"login_date\", col(\"login_date\").cast(DateType()))\n",
    "new_session_df = new_session_df.withColumn(\"logout_date\", col(\"logout_date\").cast(DateType()))\n",
    "new_session_df = new_session_df.withColumn(\"login_time\", col(\"login_time\").cast(TimestampType()))\n",
    "new_session_df = new_session_df.withColumn(\"logout_time\", col(\"logout_time\").cast(TimestampType()))\n",
    "# Show the result DataFrame\n",
    "new_session_df.show(truncate=False)\n",
    "new_session_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next block of code is to save this tables as csv files, but those files are not used in later processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7da228d-6c2f-49a5-a411-a4773a7a2239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 16:28:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(event_type),EqualTo(event_type,registration)\n",
      "23/11/18 16:28:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(event_type#43),(event_type#43 = registration)\n",
      "23/11/18 16:28:48 INFO CodeGenerator: Code generated in 11.376021 ms\n",
      "23/11/18 16:28:48 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 355.5 KiB, free 363.4 MiB)\n",
      "23/11/18 16:28:48 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 363.4 MiB)\n",
      "23/11/18 16:28:48 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 172.26.138.14:44159 (size: 35.1 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:28:48 INFO SparkContext: Created broadcast 34 from toPandas at /tmp/ipykernel_533/1718442566.py:1\n",
      "23/11/18 16:28:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/11/18 16:28:48 INFO SparkContext: Starting job: toPandas at /tmp/ipykernel_533/1718442566.py:1\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Got job 22 (toPandas at /tmp/ipykernel_533/1718442566.py:1) with 2 output partitions\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Final stage: ResultStage 29 (toPandas at /tmp/ipykernel_533/1718442566.py:1)\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[89] at toPandas at /tmp/ipykernel_533/1718442566.py:1), which has no missing parents\n",
      "23/11/18 16:28:48 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 22.1 KiB, free 363.3 MiB)\n",
      "23/11/18 16:28:48 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 9.5 KiB, free 363.3 MiB)\n",
      "23/11/18 16:28:48 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 172.26.138.14:44159 (size: 9.5 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:28:48 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1580\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 29 (MapPartitionsRDD[89] at toPandas at /tmp/ipykernel_533/1718442566.py:1) (first 15 tasks are for partitions Vector(0, 1))\n",
      "23/11/18 16:28:48 INFO TaskSchedulerImpl: Adding task set 29.0 with 2 tasks resource profile 0\n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 41) (172.26.138.14, executor driver, partition 0, PROCESS_LOCAL, 8718 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 1.0 in stage 29.0 (TID 42) (172.26.138.14, executor driver, partition 1, PROCESS_LOCAL, 8718 bytes) \n",
      "23/11/18 16:28:48 INFO Executor: Running task 1.0 in stage 29.0 (TID 42)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 0.0 in stage 29.0 (TID 41)\n",
      "23/11/18 16:28:48 INFO CodeGenerator: Code generated in 13.179464 ms\n",
      "23/11/18 16:28:48 INFO FileScanRDD: Reading File path: file:///home/markov/nordeusProject/events.jsonl, range: 4194304-7668804, partition values: [empty row]\n",
      "23/11/18 16:28:48 INFO FileScanRDD: Reading File path: file:///home/markov/nordeusProject/events.jsonl, range: 0-4194304, partition values: [empty row]\n",
      "23/11/18 16:28:48 INFO Executor: Finished task 1.0 in stage 29.0 (TID 42). 200364 bytes result sent to driver\n",
      "23/11/18 16:28:48 INFO TaskSetManager: Finished task 1.0 in stage 29.0 (TID 42) in 102 ms on 172.26.138.14 (executor driver) (1/2)\n",
      "23/11/18 16:28:48 INFO Executor: Finished task 0.0 in stage 29.0 (TID 41). 240410 bytes result sent to driver\n",
      "23/11/18 16:28:48 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 41) in 110 ms on 172.26.138.14 (executor driver) (2/2)\n",
      "23/11/18 16:28:48 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool \n",
      "23/11/18 16:28:48 INFO DAGScheduler: ResultStage 29 (toPandas at /tmp/ipykernel_533/1718442566.py:1) finished in 0.116 s\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/11/18 16:28:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Job 22 finished: toPandas at /tmp/ipykernel_533/1718442566.py:1, took 0.118973 s\n",
      "23/11/18 16:28:48 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 172.26.138.14:44159 in memory (size: 5.9 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:28:48 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 172.26.138.14:44159 in memory (size: 9.5 KiB, free: 366.0 MiB)\n",
      "/home/markov/spark/spark-3.5.0/python/pyspark/sql/pandas/types.py:563: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if not is_datetime64tz_dtype(pser.dtype):\n",
      "/home/markov/spark/spark-3.5.0/python/pyspark/sql/pandas/types.py:379: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(s.dtype):\n",
      "23/11/18 16:28:48 INFO CodeGenerator: Code generated in 12.673525 ms\n",
      "23/11/18 16:28:48 INFO SparkContext: Starting job: toPandas at /tmp/ipykernel_533/1718442566.py:2\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Got job 23 (toPandas at /tmp/ipykernel_533/1718442566.py:2) with 12 output partitions\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Final stage: ResultStage 30 (toPandas at /tmp/ipykernel_533/1718442566.py:2)\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[91] at toPandas at /tmp/ipykernel_533/1718442566.py:2), which has no missing parents\n",
      "23/11/18 16:28:48 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 18.6 KiB, free 363.4 MiB)\n",
      "23/11/18 16:28:48 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 363.4 MiB)\n",
      "23/11/18 16:28:48 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 172.26.138.14:44159 (size: 8.7 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:28:48 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1580\n",
      "23/11/18 16:28:48 INFO DAGScheduler: Submitting 12 missing tasks from ResultStage 30 (MapPartitionsRDD[91] at toPandas at /tmp/ipykernel_533/1718442566.py:2) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "23/11/18 16:28:48 INFO TaskSchedulerImpl: Adding task set 30.0 with 12 tasks resource profile 0\n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 43) (172.26.138.14, executor driver, partition 0, PROCESS_LOCAL, 95166 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 1.0 in stage 30.0 (TID 44) (172.26.138.14, executor driver, partition 1, PROCESS_LOCAL, 177662 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 2.0 in stage 30.0 (TID 45) (172.26.138.14, executor driver, partition 2, PROCESS_LOCAL, 172734 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 3.0 in stage 30.0 (TID 46) (172.26.138.14, executor driver, partition 3, PROCESS_LOCAL, 89193 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 4.0 in stage 30.0 (TID 47) (172.26.138.14, executor driver, partition 4, PROCESS_LOCAL, 166603 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 5.0 in stage 30.0 (TID 48) (172.26.138.14, executor driver, partition 5, PROCESS_LOCAL, 163804 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 6.0 in stage 30.0 (TID 49) (172.26.138.14, executor driver, partition 6, PROCESS_LOCAL, 83430 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 7.0 in stage 30.0 (TID 50) (172.26.138.14, executor driver, partition 7, PROCESS_LOCAL, 155940 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 8.0 in stage 30.0 (TID 51) (172.26.138.14, executor driver, partition 8, PROCESS_LOCAL, 149878 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 9.0 in stage 30.0 (TID 52) (172.26.138.14, executor driver, partition 9, PROCESS_LOCAL, 79390 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 10.0 in stage 30.0 (TID 53) (172.26.138.14, executor driver, partition 10, PROCESS_LOCAL, 143148 bytes) \n",
      "23/11/18 16:28:48 INFO TaskSetManager: Starting task 11.0 in stage 30.0 (TID 54) (172.26.138.14, executor driver, partition 11, PROCESS_LOCAL, 137816 bytes) \n",
      "23/11/18 16:28:48 INFO Executor: Running task 1.0 in stage 30.0 (TID 44)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 0.0 in stage 30.0 (TID 43)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 2.0 in stage 30.0 (TID 45)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 5.0 in stage 30.0 (TID 48)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 4.0 in stage 30.0 (TID 47)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 3.0 in stage 30.0 (TID 46)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 9.0 in stage 30.0 (TID 52)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 10.0 in stage 30.0 (TID 53)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 11.0 in stage 30.0 (TID 54)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 8.0 in stage 30.0 (TID 51)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 7.0 in stage 30.0 (TID 50)\n",
      "23/11/18 16:28:48 INFO Executor: Running task 6.0 in stage 30.0 (TID 49)\n",
      "23/11/18 16:28:48 INFO CodeGenerator: Code generated in 71.338554 ms\n",
      "23/11/18 16:28:49 INFO PythonRunner: Times: total = 130, boot = -608, init = 734, finish = 4\n",
      "23/11/18 16:28:49 INFO Executor: Finished task 9.0 in stage 30.0 (TID 52). 29688 bytes result sent to driver\n",
      "23/11/18 16:28:49 INFO TaskSetManager: Finished task 9.0 in stage 30.0 (TID 52) in 272 ms on 172.26.138.14 (executor driver) (1/12)\n",
      "23/11/18 16:28:49 INFO PythonRunner: Times: total = 140, boot = -604, init = 739, finish = 5\n",
      "23/11/18 16:28:49 INFO Executor: Finished task 11.0 in stage 30.0 (TID 54). 52636 bytes result sent to driver\n",
      "23/11/18 16:28:49 INFO PythonRunner: Times: total = 148, boot = -552, init = 696, finish = 4\n",
      "23/11/18 16:28:49 INFO TaskSetManager: Finished task 11.0 in stage 30.0 (TID 54) in 279 ms on 172.26.138.14 (executor driver) (2/12)\n",
      "23/11/18 16:28:49 INFO PythonRunner: Times: total = 197, boot = -587, init = 777, finish = 7\n",
      "23/11/18 16:28:49 INFO Executor: Finished task 0.0 in stage 30.0 (TID 43). 30643 bytes result sent to driver\n",
      "23/11/18 16:28:49 INFO PythonRunner: Times: total = 172, boot = -562, init = 729, finish = 5\n",
      "23/11/18 16:28:49 INFO Executor: Finished task 3.0 in stage 30.0 (TID 46). 30230 bytes result sent to driver\n",
      "23/11/18 16:28:49 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 43) in 291 ms on 172.26.138.14 (executor driver) (3/12)\n",
      "23/11/18 16:28:49 INFO Executor: Finished task 6.0 in stage 30.0 (TID 49). 30137 bytes result sent to driver\n",
      "23/11/18 16:28:49 INFO PythonRunner: Times: total = 136, boot = -552, init = 681, finish = 7\n",
      "23/11/18 16:28:49 INFO TaskSetManager: Finished task 3.0 in stage 30.0 (TID 46) in 289 ms on 172.26.138.14 (executor driver) (4/12)\n",
      "23/11/18 16:28:49 INFO TaskSetManager: Finished task 6.0 in stage 30.0 (TID 49) in 288 ms on 172.26.138.14 (executor driver) (5/12)\n",
      "23/11/18 16:28:49 INFO Executor: Finished task 1.0 in stage 30.0 (TID 44). 58214 bytes result sent to driver\n",
      "23/11/18 16:28:49 INFO TaskSetManager: Finished task 1.0 in stage 30.0 (TID 44) in 298 ms on 172.26.138.14 (executor driver) (6/12)\n",
      "23/11/18 16:28:49 INFO PythonRunner: Times: total = 163, boot = -548, init = 705, finish = 6\n",
      "23/11/18 16:28:49 INFO PythonRunner: Times: total = 170, boot = -532, init = 696, finish = 6\n",
      "23/11/18 16:28:49 INFO PythonRunner: Times: total = 176, boot = -504, init = 669, finish = 11\n",
      "23/11/18 16:28:49 INFO PythonRunner: Times: total = 177, boot = -592, init = 760, finish = 9\n",
      "23/11/18 16:28:49 INFO PythonRunner: Times: total = 143, boot = -556, init = 691, finish = 8\n",
      "23/11/18 16:28:49 INFO Executor: Finished task 8.0 in stage 30.0 (TID 51). 56908 bytes result sent to driver\n",
      "23/11/18 16:28:49 INFO Executor: Finished task 7.0 in stage 30.0 (TID 50). 58001 bytes result sent to driver\n",
      "23/11/18 16:28:49 INFO Executor: Finished task 10.0 in stage 30.0 (TID 53). 54693 bytes result sent to driver\n",
      "23/11/18 16:28:49 INFO PythonRunner: Times: total = 133, boot = -555, init = 683, finish = 5\n",
      "23/11/18 16:28:49 INFO TaskSetManager: Finished task 8.0 in stage 30.0 (TID 51) in 299 ms on 172.26.138.14 (executor driver) (7/12)\n",
      "23/11/18 16:28:49 INFO Executor: Finished task 2.0 in stage 30.0 (TID 45). 57466 bytes result sent to driver\n",
      "23/11/18 16:28:49 INFO Executor: Finished task 4.0 in stage 30.0 (TID 47). 58011 bytes result sent to driver\n",
      "23/11/18 16:28:49 INFO TaskSetManager: Finished task 10.0 in stage 30.0 (TID 53) in 299 ms on 172.26.138.14 (executor driver) (8/12)\n",
      "23/11/18 16:28:49 INFO Executor: Finished task 5.0 in stage 30.0 (TID 48). 57646 bytes result sent to driver\n",
      "23/11/18 16:28:49 INFO TaskSetManager: Finished task 2.0 in stage 30.0 (TID 45) in 304 ms on 172.26.138.14 (executor driver) (9/12)\n",
      "23/11/18 16:28:49 INFO TaskSetManager: Finished task 7.0 in stage 30.0 (TID 50) in 301 ms on 172.26.138.14 (executor driver) (10/12)\n",
      "23/11/18 16:28:49 INFO TaskSetManager: Finished task 4.0 in stage 30.0 (TID 47) in 304 ms on 172.26.138.14 (executor driver) (11/12)\n",
      "23/11/18 16:28:49 INFO TaskSetManager: Finished task 5.0 in stage 30.0 (TID 48) in 305 ms on 172.26.138.14 (executor driver) (12/12)\n",
      "23/11/18 16:28:49 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
      "23/11/18 16:28:49 INFO DAGScheduler: ResultStage 30 (toPandas at /tmp/ipykernel_533/1718442566.py:2) finished in 0.317 s\n",
      "23/11/18 16:28:49 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/11/18 16:28:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished\n",
      "23/11/18 16:28:49 INFO DAGScheduler: Job 23 finished: toPandas at /tmp/ipykernel_533/1718442566.py:2, took 0.321758 s\n",
      "/home/markov/spark/spark-3.5.0/python/pyspark/sql/pandas/types.py:563: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if not is_datetime64tz_dtype(pser.dtype):\n",
      "/home/markov/spark/spark-3.5.0/python/pyspark/sql/pandas/types.py:379: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(s.dtype):\n",
      "/home/markov/spark/spark-3.5.0/python/pyspark/sql/pandas/types.py:563: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if not is_datetime64tz_dtype(pser.dtype):\n",
      "/home/markov/spark/spark-3.5.0/python/pyspark/sql/pandas/types.py:379: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(s.dtype):\n",
      "23/11/18 16:28:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(event_type),EqualTo(event_type,transaction)\n",
      "23/11/18 16:28:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(event_type#52),(event_type#52 = transaction)\n",
      "23/11/18 16:28:50 INFO CodeGenerator: Code generated in 12.287663 ms\n",
      "23/11/18 16:28:50 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 355.5 KiB, free 363.0 MiB)\n",
      "23/11/18 16:28:50 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 363.0 MiB)\n",
      "23/11/18 16:28:50 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 172.26.138.14:44159 (size: 35.1 KiB, free: 365.9 MiB)\n",
      "23/11/18 16:28:50 INFO SparkContext: Created broadcast 37 from toPandas at /tmp/ipykernel_533/1718442566.py:3\n",
      "23/11/18 16:28:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/11/18 16:28:50 INFO SparkContext: Starting job: toPandas at /tmp/ipykernel_533/1718442566.py:3\n",
      "23/11/18 16:28:50 INFO DAGScheduler: Got job 24 (toPandas at /tmp/ipykernel_533/1718442566.py:3) with 2 output partitions\n",
      "23/11/18 16:28:50 INFO DAGScheduler: Final stage: ResultStage 31 (toPandas at /tmp/ipykernel_533/1718442566.py:3)\n",
      "23/11/18 16:28:50 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/11/18 16:28:50 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/18 16:28:50 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[95] at toPandas at /tmp/ipykernel_533/1718442566.py:3), which has no missing parents\n",
      "23/11/18 16:28:50 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 22.5 KiB, free 363.0 MiB)\n",
      "23/11/18 16:28:50 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 362.9 MiB)\n",
      "23/11/18 16:28:50 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 172.26.138.14:44159 (size: 9.8 KiB, free: 365.9 MiB)\n",
      "23/11/18 16:28:50 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1580\n",
      "23/11/18 16:28:50 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 31 (MapPartitionsRDD[95] at toPandas at /tmp/ipykernel_533/1718442566.py:3) (first 15 tasks are for partitions Vector(0, 1))\n",
      "23/11/18 16:28:50 INFO TaskSchedulerImpl: Adding task set 31.0 with 2 tasks resource profile 0\n",
      "23/11/18 16:28:50 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 55) (172.26.138.14, executor driver, partition 0, PROCESS_LOCAL, 8718 bytes) \n",
      "23/11/18 16:28:50 INFO TaskSetManager: Starting task 1.0 in stage 31.0 (TID 56) (172.26.138.14, executor driver, partition 1, PROCESS_LOCAL, 8718 bytes) \n",
      "23/11/18 16:28:50 INFO Executor: Running task 0.0 in stage 31.0 (TID 55)\n",
      "23/11/18 16:28:50 INFO Executor: Running task 1.0 in stage 31.0 (TID 56)\n",
      "23/11/18 16:28:50 INFO CodeGenerator: Code generated in 11.318521 ms\n",
      "23/11/18 16:28:50 INFO FileScanRDD: Reading File path: file:///home/markov/nordeusProject/events.jsonl, range: 4194304-7668804, partition values: [empty row]\n",
      "23/11/18 16:28:50 INFO FileScanRDD: Reading File path: file:///home/markov/nordeusProject/events.jsonl, range: 0-4194304, partition values: [empty row]\n",
      "23/11/18 16:28:50 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 172.26.138.14:44159 in memory (size: 8.7 KiB, free: 365.9 MiB)\n",
      "23/11/18 16:28:50 INFO Executor: Finished task 1.0 in stage 31.0 (TID 56). 8851 bytes result sent to driver\n",
      "23/11/18 16:28:50 INFO TaskSetManager: Finished task 1.0 in stage 31.0 (TID 56) in 73 ms on 172.26.138.14 (executor driver) (1/2)\n",
      "23/11/18 16:28:50 INFO Executor: Finished task 0.0 in stage 31.0 (TID 55). 11305 bytes result sent to driver\n",
      "23/11/18 16:28:50 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 55) in 82 ms on 172.26.138.14 (executor driver) (2/2)\n",
      "23/11/18 16:28:50 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "23/11/18 16:28:50 INFO DAGScheduler: ResultStage 31 (toPandas at /tmp/ipykernel_533/1718442566.py:3) finished in 0.087 s\n",
      "23/11/18 16:28:50 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/11/18 16:28:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished\n",
      "23/11/18 16:28:50 INFO DAGScheduler: Job 24 finished: toPandas at /tmp/ipykernel_533/1718442566.py:3, took 0.089674 s\n",
      "/home/markov/spark/spark-3.5.0/python/pyspark/sql/pandas/types.py:563: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if not is_datetime64tz_dtype(pser.dtype):\n",
      "/home/markov/spark/spark-3.5.0/python/pyspark/sql/pandas/types.py:379: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(s.dtype):\n"
     ]
    }
   ],
   "source": [
    "df_registration = new_registration_df.toPandas()\n",
    "df_session = new_session_df.toPandas()\n",
    "df_transaction = new_transaction_df.toPandas()\n",
    "df_registration.to_csv('registration.csv')\n",
    "df_session.to_csv('session.csv')\n",
    "df_transaction.to_csv('transaction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of 3 tables are saved in postgres sql. In order to do the same there should be docker instance or some other instance of postgres sql setup with same configuration as written in next line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25e5415b-4bba-4422-a2e4-78d0b77e469b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 16:28:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(event_type),EqualTo(event_type,registration)\n",
      "23/11/18 16:28:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(event_type#43),(event_type#43 = registration)\n",
      "23/11/18 16:28:50 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 355.5 KiB, free 362.6 MiB)\n",
      "23/11/18 16:28:50 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 362.6 MiB)\n",
      "23/11/18 16:28:50 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 172.26.138.14:44159 (size: 35.1 KiB, free: 365.9 MiB)\n",
      "23/11/18 16:28:50 INFO SparkContext: Created broadcast 39 from save at NativeMethodAccessorImpl.java:0\n",
      "23/11/18 16:28:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/11/18 16:28:50 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "23/11/18 16:28:50 INFO DAGScheduler: Got job 25 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "23/11/18 16:28:50 INFO DAGScheduler: Final stage: ResultStage 32 (save at NativeMethodAccessorImpl.java:0)\n",
      "23/11/18 16:28:50 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/11/18 16:28:50 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/18 16:28:50 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[101] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/11/18 16:28:50 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 46.6 KiB, free 362.5 MiB)\n",
      "23/11/18 16:28:50 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 20.3 KiB, free 362.5 MiB)\n",
      "23/11/18 16:28:50 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 172.26.138.14:44159 (size: 20.3 KiB, free: 365.9 MiB)\n",
      "23/11/18 16:28:50 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1580\n",
      "23/11/18 16:28:50 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 32 (MapPartitionsRDD[101] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "23/11/18 16:28:50 INFO TaskSchedulerImpl: Adding task set 32.0 with 2 tasks resource profile 0\n",
      "23/11/18 16:28:50 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 57) (172.26.138.14, executor driver, partition 0, PROCESS_LOCAL, 8718 bytes) \n",
      "23/11/18 16:28:50 INFO TaskSetManager: Starting task 1.0 in stage 32.0 (TID 58) (172.26.138.14, executor driver, partition 1, PROCESS_LOCAL, 8718 bytes) \n",
      "23/11/18 16:28:50 INFO Executor: Running task 0.0 in stage 32.0 (TID 57)\n",
      "23/11/18 16:28:50 INFO Executor: Running task 1.0 in stage 32.0 (TID 58)\n",
      "23/11/18 16:28:50 INFO CodeGenerator: Code generated in 8.084593 ms\n",
      "23/11/18 16:28:50 INFO FileScanRDD: Reading File path: file:///home/markov/nordeusProject/events.jsonl, range: 4194304-7668804, partition values: [empty row]\n",
      "23/11/18 16:28:50 INFO FileScanRDD: Reading File path: file:///home/markov/nordeusProject/events.jsonl, range: 0-4194304, partition values: [empty row]\n",
      "23/11/18 16:28:51 INFO Executor: Finished task 1.0 in stage 32.0 (TID 58). 1648 bytes result sent to driver\n",
      "23/11/18 16:28:51 INFO TaskSetManager: Finished task 1.0 in stage 32.0 (TID 58) in 381 ms on 172.26.138.14 (executor driver) (1/2)\n",
      "23/11/18 16:28:51 INFO Executor: Finished task 0.0 in stage 32.0 (TID 57). 1648 bytes result sent to driver\n",
      "23/11/18 16:28:51 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 57) in 412 ms on 172.26.138.14 (executor driver) (2/2)\n",
      "23/11/18 16:28:51 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool \n",
      "23/11/18 16:28:51 INFO DAGScheduler: ResultStage 32 (save at NativeMethodAccessorImpl.java:0) finished in 0.450 s\n",
      "23/11/18 16:28:51 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/11/18 16:28:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished\n",
      "23/11/18 16:28:51 INFO DAGScheduler: Job 25 finished: save at NativeMethodAccessorImpl.java:0, took 0.452497 s\n",
      "23/11/18 16:28:51 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "23/11/18 16:28:51 INFO DAGScheduler: Got job 26 (save at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "23/11/18 16:28:51 INFO DAGScheduler: Final stage: ResultStage 33 (save at NativeMethodAccessorImpl.java:0)\n",
      "23/11/18 16:28:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/11/18 16:28:51 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/18 16:28:51 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[105] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/11/18 16:28:51 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 42.0 KiB, free 362.5 MiB)\n",
      "23/11/18 16:28:51 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 362.5 MiB)\n",
      "23/11/18 16:28:51 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 172.26.138.14:44159 (size: 19.1 KiB, free: 365.9 MiB)\n",
      "23/11/18 16:28:51 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1580\n",
      "23/11/18 16:28:51 INFO DAGScheduler: Submitting 12 missing tasks from ResultStage 33 (MapPartitionsRDD[105] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "23/11/18 16:28:51 INFO TaskSchedulerImpl: Adding task set 33.0 with 12 tasks resource profile 0\n",
      "23/11/18 16:28:51 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 59) (172.26.138.14, executor driver, partition 0, PROCESS_LOCAL, 95166 bytes) \n",
      "23/11/18 16:28:51 INFO TaskSetManager: Starting task 1.0 in stage 33.0 (TID 60) (172.26.138.14, executor driver, partition 1, PROCESS_LOCAL, 177662 bytes) \n",
      "23/11/18 16:28:51 INFO TaskSetManager: Starting task 2.0 in stage 33.0 (TID 61) (172.26.138.14, executor driver, partition 2, PROCESS_LOCAL, 172734 bytes) \n",
      "23/11/18 16:28:51 INFO TaskSetManager: Starting task 3.0 in stage 33.0 (TID 62) (172.26.138.14, executor driver, partition 3, PROCESS_LOCAL, 89193 bytes) \n",
      "23/11/18 16:28:51 INFO TaskSetManager: Starting task 4.0 in stage 33.0 (TID 63) (172.26.138.14, executor driver, partition 4, PROCESS_LOCAL, 166603 bytes) \n",
      "23/11/18 16:28:51 INFO TaskSetManager: Starting task 5.0 in stage 33.0 (TID 64) (172.26.138.14, executor driver, partition 5, PROCESS_LOCAL, 163804 bytes) \n",
      "23/11/18 16:28:51 INFO TaskSetManager: Starting task 6.0 in stage 33.0 (TID 65) (172.26.138.14, executor driver, partition 6, PROCESS_LOCAL, 83430 bytes) \n",
      "23/11/18 16:28:51 INFO TaskSetManager: Starting task 7.0 in stage 33.0 (TID 66) (172.26.138.14, executor driver, partition 7, PROCESS_LOCAL, 155940 bytes) \n",
      "23/11/18 16:28:51 INFO TaskSetManager: Starting task 8.0 in stage 33.0 (TID 67) (172.26.138.14, executor driver, partition 8, PROCESS_LOCAL, 149878 bytes) \n",
      "23/11/18 16:28:51 INFO TaskSetManager: Starting task 9.0 in stage 33.0 (TID 68) (172.26.138.14, executor driver, partition 9, PROCESS_LOCAL, 79390 bytes) \n",
      "23/11/18 16:28:51 INFO TaskSetManager: Starting task 10.0 in stage 33.0 (TID 69) (172.26.138.14, executor driver, partition 10, PROCESS_LOCAL, 143148 bytes) \n",
      "23/11/18 16:28:51 INFO TaskSetManager: Starting task 11.0 in stage 33.0 (TID 70) (172.26.138.14, executor driver, partition 11, PROCESS_LOCAL, 137816 bytes) \n",
      "23/11/18 16:28:51 INFO Executor: Running task 0.0 in stage 33.0 (TID 59)\n",
      "23/11/18 16:28:51 INFO Executor: Running task 3.0 in stage 33.0 (TID 62)\n",
      "23/11/18 16:28:51 INFO Executor: Running task 1.0 in stage 33.0 (TID 60)\n",
      "23/11/18 16:28:51 INFO Executor: Running task 4.0 in stage 33.0 (TID 63)\n",
      "23/11/18 16:28:51 INFO Executor: Running task 2.0 in stage 33.0 (TID 61)\n",
      "23/11/18 16:28:51 INFO Executor: Running task 5.0 in stage 33.0 (TID 64)\n",
      "23/11/18 16:28:51 INFO Executor: Running task 8.0 in stage 33.0 (TID 67)\n",
      "23/11/18 16:28:51 INFO Executor: Running task 9.0 in stage 33.0 (TID 68)\n",
      "23/11/18 16:28:51 INFO Executor: Running task 7.0 in stage 33.0 (TID 66)\n",
      "23/11/18 16:28:51 INFO Executor: Running task 11.0 in stage 33.0 (TID 70)\n",
      "23/11/18 16:28:51 INFO Executor: Running task 10.0 in stage 33.0 (TID 69)\n",
      "23/11/18 16:28:51 INFO Executor: Running task 6.0 in stage 33.0 (TID 65)\n",
      "23/11/18 16:28:51 INFO CodeGenerator: Code generated in 152.374891 ms\n",
      "23/11/18 16:28:51 INFO PythonRunner: Times: total = 182, boot = -2218, init = 2396, finish = 4\n",
      "23/11/18 16:28:51 INFO Executor: Finished task 3.0 in stage 33.0 (TID 62). 1664 bytes result sent to driver\n",
      "23/11/18 16:28:51 INFO TaskSetManager: Finished task 3.0 in stage 33.0 (TID 62) in 340 ms on 172.26.138.14 (executor driver) (1/12)\n",
      "23/11/18 16:28:51 INFO PythonRunner: Times: total = 173, boot = -2170, init = 2339, finish = 4\n",
      "23/11/18 16:28:51 INFO PythonRunner: Times: total = 212, boot = -2200, init = 2407, finish = 5\n",
      "23/11/18 16:28:51 INFO PythonRunner: Times: total = 137, boot = -2142, init = 2273, finish = 6\n",
      "23/11/18 16:28:51 INFO PythonRunner: Times: total = 130, boot = -2207, init = 2332, finish = 5\n",
      "23/11/18 16:28:51 INFO Executor: Finished task 6.0 in stage 33.0 (TID 65). 1664 bytes result sent to driver\n",
      "23/11/18 16:28:51 INFO Executor: Finished task 0.0 in stage 33.0 (TID 59). 1664 bytes result sent to driver\n",
      "23/11/18 16:28:51 INFO TaskSetManager: Finished task 6.0 in stage 33.0 (TID 65) in 372 ms on 172.26.138.14 (executor driver) (2/12)\n",
      "23/11/18 16:28:51 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 59) in 375 ms on 172.26.138.14 (executor driver) (3/12)\n",
      "23/11/18 16:28:51 INFO Executor: Finished task 9.0 in stage 33.0 (TID 68). 1664 bytes result sent to driver\n",
      "23/11/18 16:28:51 INFO Executor: Finished task 10.0 in stage 33.0 (TID 69). 1664 bytes result sent to driver\n",
      "23/11/18 16:28:51 INFO TaskSetManager: Finished task 9.0 in stage 33.0 (TID 68) in 377 ms on 172.26.138.14 (executor driver) (4/12)\n",
      "23/11/18 16:28:51 INFO TaskSetManager: Finished task 10.0 in stage 33.0 (TID 69) in 377 ms on 172.26.138.14 (executor driver) (5/12)\n",
      "23/11/18 16:28:51 INFO PythonRunner: Times: total = 197, boot = -2167, init = 2354, finish = 10\n",
      "23/11/18 16:28:51 INFO PythonRunner: Times: total = 153, boot = -2194, init = 2338, finish = 9\n",
      "23/11/18 16:28:51 INFO PythonRunner: Times: total = 197, boot = -2162, init = 2353, finish = 6\n",
      "23/11/18 16:28:51 INFO PythonRunner: Times: total = 135, boot = -2208, init = 2337, finish = 6\n",
      "23/11/18 16:28:51 INFO Executor: Finished task 4.0 in stage 33.0 (TID 63). 1664 bytes result sent to driver\n",
      "23/11/18 16:28:51 INFO Executor: Finished task 7.0 in stage 33.0 (TID 66). 1664 bytes result sent to driver\n",
      "23/11/18 16:28:51 INFO PythonRunner: Times: total = 163, boot = -2179, init = 2335, finish = 7\n",
      "23/11/18 16:28:51 INFO Executor: Finished task 2.0 in stage 33.0 (TID 61). 1664 bytes result sent to driver\n",
      "23/11/18 16:28:51 INFO TaskSetManager: Finished task 4.0 in stage 33.0 (TID 63) in 405 ms on 172.26.138.14 (executor driver) (6/12)\n",
      "23/11/18 16:28:51 INFO TaskSetManager: Finished task 7.0 in stage 33.0 (TID 66) in 405 ms on 172.26.138.14 (executor driver) (7/12)\n",
      "23/11/18 16:28:51 INFO TaskSetManager: Finished task 2.0 in stage 33.0 (TID 61) in 409 ms on 172.26.138.14 (executor driver) (8/12)\n",
      "23/11/18 16:28:51 INFO Executor: Finished task 5.0 in stage 33.0 (TID 64). 1664 bytes result sent to driver\n",
      "23/11/18 16:28:51 INFO TaskSetManager: Finished task 5.0 in stage 33.0 (TID 64) in 408 ms on 172.26.138.14 (executor driver) (9/12)\n",
      "23/11/18 16:28:51 INFO PythonRunner: Times: total = 130, boot = -2192, init = 2317, finish = 5\n",
      "23/11/18 16:28:51 INFO PythonRunner: Times: total = 261, boot = -2166, init = 2419, finish = 8\n",
      "23/11/18 16:28:51 INFO Executor: Finished task 8.0 in stage 33.0 (TID 67). 1664 bytes result sent to driver\n",
      "23/11/18 16:28:51 INFO TaskSetManager: Finished task 8.0 in stage 33.0 (TID 67) in 412 ms on 172.26.138.14 (executor driver) (10/12)\n",
      "23/11/18 16:28:51 INFO Executor: Finished task 1.0 in stage 33.0 (TID 60). 1664 bytes result sent to driver\n",
      "23/11/18 16:28:51 INFO Executor: Finished task 11.0 in stage 33.0 (TID 70). 1664 bytes result sent to driver\n",
      "23/11/18 16:28:51 INFO TaskSetManager: Finished task 1.0 in stage 33.0 (TID 60) in 420 ms on 172.26.138.14 (executor driver) (11/12)\n",
      "23/11/18 16:28:51 INFO TaskSetManager: Finished task 11.0 in stage 33.0 (TID 70) in 415 ms on 172.26.138.14 (executor driver) (12/12)\n",
      "23/11/18 16:28:51 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
      "23/11/18 16:28:51 INFO DAGScheduler: ResultStage 33 (save at NativeMethodAccessorImpl.java:0) finished in 0.444 s\n",
      "23/11/18 16:28:51 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/11/18 16:28:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished\n",
      "23/11/18 16:28:51 INFO DAGScheduler: Job 26 finished: save at NativeMethodAccessorImpl.java:0, took 0.448796 s\n",
      "23/11/18 16:28:51 INFO FileSourceStrategy: Pushed Filters: IsNotNull(event_type),EqualTo(event_type,transaction)\n",
      "23/11/18 16:28:51 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(event_type#52),(event_type#52 = transaction)\n",
      "23/11/18 16:28:51 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 355.5 KiB, free 362.1 MiB)\n",
      "23/11/18 16:28:51 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 362.1 MiB)\n",
      "23/11/18 16:28:51 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 172.26.138.14:44159 (size: 35.1 KiB, free: 365.8 MiB)\n",
      "23/11/18 16:28:51 INFO SparkContext: Created broadcast 42 from save at NativeMethodAccessorImpl.java:0\n",
      "23/11/18 16:28:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/11/18 16:28:51 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "23/11/18 16:28:51 INFO DAGScheduler: Got job 27 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "23/11/18 16:28:51 INFO DAGScheduler: Final stage: ResultStage 34 (save at NativeMethodAccessorImpl.java:0)\n",
      "23/11/18 16:28:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/11/18 16:28:51 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/18 16:28:51 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[111] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/11/18 16:28:51 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 49.0 KiB, free 362.0 MiB)\n",
      "23/11/18 16:28:51 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 21.5 KiB, free 362.0 MiB)\n",
      "23/11/18 16:28:51 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 172.26.138.14:44159 (size: 21.5 KiB, free: 365.8 MiB)\n",
      "23/11/18 16:28:51 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1580\n",
      "23/11/18 16:28:51 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 34 (MapPartitionsRDD[111] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "23/11/18 16:28:51 INFO TaskSchedulerImpl: Adding task set 34.0 with 2 tasks resource profile 0\n",
      "23/11/18 16:28:51 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 71) (172.26.138.14, executor driver, partition 0, PROCESS_LOCAL, 8718 bytes) \n",
      "23/11/18 16:28:51 INFO TaskSetManager: Starting task 1.0 in stage 34.0 (TID 72) (172.26.138.14, executor driver, partition 1, PROCESS_LOCAL, 8718 bytes) \n",
      "23/11/18 16:28:51 INFO Executor: Running task 0.0 in stage 34.0 (TID 71)\n",
      "23/11/18 16:28:51 INFO Executor: Running task 1.0 in stage 34.0 (TID 72)\n",
      "23/11/18 16:28:51 INFO CodeGenerator: Code generated in 7.744303 ms\n",
      "23/11/18 16:28:51 INFO FileScanRDD: Reading File path: file:///home/markov/nordeusProject/events.jsonl, range: 0-4194304, partition values: [empty row]\n",
      "23/11/18 16:28:51 INFO FileScanRDD: Reading File path: file:///home/markov/nordeusProject/events.jsonl, range: 4194304-7668804, partition values: [empty row]\n",
      "23/11/18 16:28:51 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 172.26.138.14:44159 in memory (size: 19.1 KiB, free: 365.8 MiB)\n",
      "23/11/18 16:28:51 INFO Executor: Finished task 1.0 in stage 34.0 (TID 72). 1648 bytes result sent to driver\n",
      "23/11/18 16:28:51 INFO TaskSetManager: Finished task 1.0 in stage 34.0 (TID 72) in 97 ms on 172.26.138.14 (executor driver) (1/2)\n",
      "23/11/18 16:28:51 INFO Executor: Finished task 0.0 in stage 34.0 (TID 71). 1648 bytes result sent to driver\n",
      "23/11/18 16:28:51 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 71) in 103 ms on 172.26.138.14 (executor driver) (2/2)\n",
      "23/11/18 16:28:51 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool \n",
      "23/11/18 16:28:51 INFO DAGScheduler: ResultStage 34 (save at NativeMethodAccessorImpl.java:0) finished in 0.127 s\n",
      "23/11/18 16:28:51 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/11/18 16:28:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished\n",
      "23/11/18 16:28:51 INFO DAGScheduler: Job 27 finished: save at NativeMethodAccessorImpl.java:0, took 0.129820 s\n",
      "23/11/18 16:57:01 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.26.138.14:44159 in memory (size: 35.1 KiB, free: 365.9 MiB)\n",
      "23/11/18 16:57:01 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.26.138.14:44159 in memory (size: 35.1 KiB, free: 365.9 MiB)\n",
      "23/11/18 16:57:01 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 172.26.138.14:44159 in memory (size: 9.9 KiB, free: 365.9 MiB)\n",
      "23/11/18 16:57:01 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 172.26.138.14:44159 in memory (size: 35.1 KiB, free: 365.9 MiB)\n",
      "23/11/18 16:57:01 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.26.138.14:44159 in memory (size: 22.0 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:57:01 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 172.26.138.14:44159 in memory (size: 20.3 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:57:01 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 172.26.138.14:44159 in memory (size: 8.4 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:57:01 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.26.138.14:44159 in memory (size: 35.1 KiB, free: 366.0 MiB)\n",
      "23/11/18 16:57:01 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.26.138.14:44159 in memory (size: 35.1 KiB, free: 366.1 MiB)\n",
      "23/11/18 16:57:01 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.26.138.14:44159 in memory (size: 35.1 KiB, free: 366.1 MiB)\n",
      "23/11/18 16:57:01 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 172.26.138.14:44159 in memory (size: 9.8 KiB, free: 366.1 MiB)\n",
      "23/11/18 16:57:01 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.26.138.14:44159 in memory (size: 10.1 KiB, free: 366.1 MiB)\n",
      "23/11/18 16:57:01 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 172.26.138.14:44159 in memory (size: 21.5 KiB, free: 366.1 MiB)\n",
      "23/11/18 16:57:01 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 172.26.138.14:44159 in memory (size: 35.1 KiB, free: 366.2 MiB)\n",
      "23/11/18 16:57:01 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 172.26.138.14:44159 in memory (size: 15.4 KiB, free: 366.2 MiB)\n",
      "23/11/18 16:57:01 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.26.138.14:44159 in memory (size: 9.3 KiB, free: 366.2 MiB)\n",
      "23/11/18 16:57:01 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.26.138.14:44159 in memory (size: 35.1 KiB, free: 366.2 MiB)\n"
     ]
    }
   ],
   "source": [
    "jdbc_url = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"nordeus\",\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "\n",
    "new_registration_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"registration\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"user\", properties[\"user\"]) \\\n",
    "    .option(\"password\", properties[\"password\"]) \\\n",
    "    .option(\"driver\", properties[\"driver\"]) \\\n",
    "    .save()\n",
    "\n",
    "new_session_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"session\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"user\", properties[\"user\"]) \\\n",
    "    .option(\"password\", properties[\"password\"]) \\\n",
    "    .option(\"driver\", properties[\"driver\"]) \\\n",
    "    .save()\n",
    "\n",
    "new_transaction_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"transaction\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"user\", properties[\"user\"]) \\\n",
    "    .option(\"password\", properties[\"password\"]) \\\n",
    "    .option(\"driver\", properties[\"driver\"]) \\\n",
    "    .save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
